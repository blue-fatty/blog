---
title: Keras
tags: [keras]
slug: keras
date: 2017-12-04
---

Record the problems encountered while using Keras and their solutions. The version of Keras I used is `2.0.8`.

<!--more-->

## Problems encountered

### 2017-12-04 Â· Get variable's value in middle layer

Reference: [How can I get hidden layer representation of the given data? #41](https://github.com/fchollet/keras/issues/41)

The simplest way is using the same code of the original model, and

1. replace the output with the variable you want
2. `new_model.set_weights(trained_model.get_weights())`
3. `new_model.predict(input_data, batch_size=32)`

Note the `batch_size` is import for large amount of samples. The `K.function()` mentioned in the [issue #41](https://github.com/fchollet/keras/issues/41) raised `OOM exception`. Of course you can split data into batches by yourself and use the `K.function()` method, but the method showed above is more convinient for me in my case.

Example case:

I want to get the output value of pooling layer.

- `get_model()` function return a model for training
- `train_model()`
- `get_p_out()` function almost have the same code with `get_model()` except 
	- the input parameters
	- the `Model()`'s output parameter
	- `set_weights()` from trained model
	- use the new model to predict value

``` python
def get_model(input_shape=(64, 64, 3), kernel_num=64):
    
    inputs = Input(shape=input_shape)
    
    c_out = Conv2D(filters=kernel_num, kernel_size=(3, 3), activation='relu', \
                   use_bias=True, padding='same')(inputs)
    p_out = MaxPooling2D((2, 2), padding='same')(c_out)
    flat  = Flatten()(p_out)
    d_out = Dense(class_cnt, activation='softmax')(flat)
    
    model = Model(inputs, d_out)
    opt = Adam()
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc'])
    
    return model
```

``` python
def train_model(model, batch_size=32):
    
    model_path = 'model.h5'
    model_checkpoint = ModelCheckpoint(model_path, monitor='val_acc', save_best_only=True, save_weights_only=True)
    early_stopping = EarlyStopping(patience=5, monitor='val_acc')
    hist = model.fit(X_train, y_train, batch_size=batch_size, epochs=100, \
                     validation_data=[X_test, y_test], callbacks=[model_checkpoint, early_stopping])
    
    model.load_weights(model_path)
    
    preds = model.evaluate(X_test, y_test)
    
    print 'Test loss:', preds[0]
    print 'Test accuracy:', preds[1]
    
    return model
```

``` python
def get_p_out(model_trained, input_data, input_shape=(64, 64, 3), kernel_num=64):
    
    inputs = Input(shape=input_shape)
    
    c_out = Conv2D(filters=kernel_num, kernel_size=(3, 3), activation='relu', \
                   use_bias=True, padding='same', )(inputs)
    p_out = MaxPooling2D((2, 2), padding='same')(c_out)
    flat  = Flatten()(p_out)
    d_out = Dense(class_cnt, activation='softmax')(flat)
    
    model = Model(inputs, p_out)
    opt = Adam()
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc'])
    
    model.set_weights(model_trained.get_weights())
    
    pred = model.predict(input_data, batch_size=32)
    
    return pred
```
