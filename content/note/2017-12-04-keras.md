---
title: Keras
tags: [keras]
slug: keras
date: 2017-12-04
---

Record the problems encountered while using Keras and their solutions. The version of Keras I used is `2.0.8`.


## Basic

### Metric

As of Keras 2.0, precision and recall were removed from the master branch. We need to implement them.

[issue #5400](https://github.com/fchollet/keras/issues/5400) or [Removed batchwise metrics](https://github.com/fchollet/keras/commit/a56b1a55182acf061b1eb2e2c86b48193a0e88f7)

<!--more-->

``` python
from keras import backend as K

 def mcor(y_true, y_pred):
     #matthews_correlation
     y_pred_pos = K.round(K.clip(y_pred, 0, 1))
     y_pred_neg = 1 - y_pred_pos
 
     y_pos = K.round(K.clip(y_true, 0, 1))
     y_neg = 1 - y_pos

     tp = K.sum(y_pos * y_pred_pos)
     tn = K.sum(y_neg * y_pred_neg)

     fp = K.sum(y_neg * y_pred_pos)
     fn = K.sum(y_pos * y_pred_neg)

     numerator = (tp * tn - fp * fn)
     denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))

     return numerator / (denominator + K.epsilon())

def precision(y_true, y_pred):
    """Precision metric.

    Only computes a batch-wise average of precision.

    Computes the precision, a metric for multi-label classification of
    how many selected items are relevant.
    """
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def recall(y_true, y_pred):
    """Recall metric.

    Only computes a batch-wise average of recall.

    Computes the recall, a metric for multi-label classification of
    how many relevant items are selected.
    """
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def f1(y_true, y_pred):
    def recall(y_true, y_pred):
        """Recall metric.

        Only computes a batch-wise average of recall.

        Computes the recall, a metric for multi-label classification of
        how many relevant items are selected.
        """
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

    def precision(y_true, y_pred):
        """Precision metric.

        Only computes a batch-wise average of precision.

        Computes the precision, a metric for multi-label classification of
        how many selected items are relevant.
        """
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision
    precision = precision(y_true, y_pred)
    recall = recall(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall))

#you can use it like this
model.compile(loss='binary_crossentropy',
              optimizer= "adam",
              metrics=[mcor, recall, f1])
```

## Problems encountered

### 2017-12-04 Â· Get variable's value in middle layer

Reference: [How can I get hidden layer representation of the given data? #41](https://github.com/fchollet/keras/issues/41)

The simplest way is using the same code of the original model, and

1. replace the output with the variable you want
2. `new_model.set_weights(trained_model.get_weights())`
3. `new_model.predict(input_data, batch_size=32)`

Note the `batch_size` is import for large amount of samples. The `K.function()` mentioned in the [issue #41](https://github.com/fchollet/keras/issues/41) raised `OOM exception`. Of course you can split data into batches by yourself and use the `K.function()` method, but the method showed above is more convinient for me in my case.

Example case:

I want to get the output value of pooling layer.

- `get_model()` function return a model for training
- `train_model()`
- `get_p_out()` function almost have the same code with `get_model()` except
	- the input parameters
	- the `Model()`'s output parameter
	- `set_weights()` from trained model
	- use the new model to predict value

``` python
def get_model(input_shape=(64, 64, 3), kernel_num=64):
    
    inputs = Input(shape=input_shape)
    
    c_out = Conv2D(filters=kernel_num, kernel_size=(3, 3), activation='relu',
                   use_bias=True, padding='same')(inputs)
    p_out = MaxPooling2D((2, 2), padding='same')(c_out)
    flat  = Flatten()(p_out)
    d_out = Dense(class_cnt, activation='softmax')(flat)
    
    model = Model(inputs, d_out)
    opt = Adam()
    model.compile(optimizer=opt, loss='categorical_crossentropy', 
                  metrics=['acc'])
    
    return model
```

``` python
def train_model(model, batch_size=32):
    
    model_path = 'model.h5'
    model_checkpoint = ModelCheckpoint(model_path, monitor='val_acc',
                                       save_best_only=True, 
                                       save_weights_only=True)
    early_stopping = EarlyStopping(patience=5, monitor='val_acc')
    hist = model.fit(X_train, y_train, batch_size=batch_size, epochs=100,
                     validation_data=[X_test, y_test], 
                     callbacks=[model_checkpoint, early_stopping])
    
    model.load_weights(model_path)
    
    preds = model.evaluate(X_test, y_test)
    
    print 'Test loss:', preds[0]
    print 'Test accuracy:', preds[1]
    
    return model
```

``` python
def get_p_out(model_trained, input_data, 
              input_shape=(64, 64, 3), kernel_num=64):
    
    inputs = Input(shape=input_shape)
    
    c_out = Conv2D(filters=kernel_num, kernel_size=(3, 3), activation='relu',
                   use_bias=True, padding='same', )(inputs)
    p_out = MaxPooling2D((2, 2), padding='same')(c_out)
    flat  = Flatten()(p_out)
    d_out = Dense(class_cnt, activation='softmax')(flat)
    
    model = Model(inputs, p_out)
    opt = Adam()
    model.compile(optimizer=opt, loss='categorical_crossentropy',
                  metrics=['acc'])
    
    model.set_weights(model_trained.get_weights())
    
    pred = model.predict(input_data, batch_size=32)
    
    return pred
```
